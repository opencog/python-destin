{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Necesary Imports\n",
      "import cPickle as pickle\n",
      "from sklearn import svm\n",
      "from load_data import *\n",
      "from network import *\n",
      "import scipy.io as io"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# *****Define Parameters for the Network and nodes\n",
      "\n",
      "# Network Params\n",
      "num_layers = 4\n",
      "patch_mode = 'Adjacent'\n",
      "image_type = 'Color'\n",
      "network_mode = True\n",
      "# For a Node: specify Your Algorithm Choice and Corresponding parameters\n",
      "\n",
      "# ******************************************************************************************\n",
      "#\n",
      "#                           Incremental Clustering\n",
      "#\n",
      "num_nodes_per_layer = [[8, 8], [4, 4], [2, 2], [1, 1]]\n",
      "num_cents_per_layer = [25, 25, 25, 25]\n",
      "print \"Uniform DeSTIN with Clustering\"\n",
      "algorithm_choice = 'Clustering'\n",
      "alg_params = {'mr': 0.01, 'vr': 0.01, 'sr': 0.001, 'DIMS': [],\n",
      "             'CENTS': [], 'node_id': [],\n",
      "             'num_cents_per_layer': num_cents_per_layer}\n",
      "# ******************************************************************************************\n",
      "'''\n",
      "#  ******************************************************************************************\n",
      "\n",
      "#           Hierarchy Of AutoEncoders\n",
      "\n",
      "print \"Uniform DeSTIN with AutoEncoders\"\n",
      "num_nodes_per_layer = [[8, 8], [4, 4], [2, 2], [1, 1]]\n",
      "num_cents_per_layer = [25, 25, 25, 25]\n",
      "algorithm_choice = 'AutoEncoder'\n",
      "inp_size = 48\n",
      "hid_size = 100\n",
      "alg_params = [[inp_size, hid_size], [4 * hid_size, hid_size],\n",
      "             [4 * hid_size, hid_size], [4 * hid_size, hid_size]]\n",
      "#  ******************************************************************************************\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Uniform DeSTIN with Clustering\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "'\\n#  ******************************************************************************************\\n\\n#           Hierarchy Of AutoEncoders\\n\\nprint \"Uniform DeSTIN with AutoEncoders\"\\nnum_nodes_per_layer = [[8, 8], [4, 4], [2, 2], [1, 1]]\\nnum_cents_per_layer = [25, 25, 25, 25]\\nalgorithm_choice = \\'AutoEncoder\\'\\ninp_size = 48\\nhid_size = 100\\nalg_params = [[inp_size, hid_size], [4 * hid_size, hid_size],\\n             [4 * hid_size, hid_size], [4 * hid_size, hid_size]]\\n#  ******************************************************************************************\\n'"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Load Data, 10 loads 5 batches in total 50,000\n",
      "# 1 to 5 load batch_1 to batch_5training images, 1 to five \n",
      "[data, labels] = loadCifar(10)\n",
      "del labels"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Declare a Network Object and load Training Data\n",
      "DESTIN = Network(\n",
      "    num_layers, algorithm_choice, alg_params, num_nodes_per_layer, patch_mode, image_type, load_cifar(4))\n",
      "DESTIN.setmode(network_mode)\n",
      "DESTIN.set_lowest_layer(0)\n",
      "# Load Data\n",
      "# Modify the location of the training data in file \"load_data.py\"\n",
      "\n",
      "# data = np.random.rand(5,32*32*3)\n",
      "# Initialize Network; there is is also a layer-wise initialization option\n",
      "DESTIN.init_network()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Train the Network\n",
      "print \"DeSTIN Training/with out Feature extraction\"\n",
      "for I in range(data.shape[0]):  # For Every image in the data set\n",
      "    if I % 200 == 0:\n",
      "        print(\"Training Iteration Number %d\" % I)\n",
      "    for L in range(DESTIN.number_of_layers):\n",
      "        if L == 0:\n",
      "            img = data[I][:].reshape(32, 32, 3)\n",
      "            DESTIN.layers[0][L].train_typical_node(img, [4, 4], algorithm_choice)\n",
      "            # This is equivalent to sharing centroids or kernels\n",
      "            DESTIN.layers[0][L].share_learned_parameters()\n",
      "            DESTIN.layers[0][L].load_input(img, [4, 4])\n",
      "            DESTIN.layers[0][L].do_layer_learning()\n",
      "        else:\n",
      "            DESTIN.layers[0][L].train_typical_node(\n",
      "                DESTIN.layers[0][L - 1].nodes, [2, 2], algorithm_choice)\n",
      "            DESTIN.layers[0][L].share_learned_parameters()\n",
      "            DESTIN.layers[0][L].load_input(\n",
      "                DESTIN.layers[0][L - 1].nodes, [2, 2])\n",
      "            DESTIN.layers[0][L].do_layer_learning()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "UnboundLocalError",
       "evalue": "local variable 'Patch' referenced before assignment",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-8-ffbb3acb6046>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mDESTIN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_typical_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm_choice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0;31m# This is equivalent to sharing centroids or kernels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mDESTIN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshare_learned_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/habtegebrial/Desktop/python-destin/DeSTIN/python-destin/layer.pyc\u001b[0m in \u001b[0;36mtrain_typical_node\u001b[0;34m(self, input_, windowSize, algorithm_choice)\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mJ\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                     TN.load_input(\n\u001b[0;32m---> 69\u001b[0;31m                         return_node_input(input_, [I, J], H, self.patch_mode, self.image_type))\n\u001b[0m\u001b[1;32m     70\u001b[0m                     \u001b[0mTN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_node_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/habtegebrial/Desktop/python-destin/DeSTIN/python-destin/load_data.pyc\u001b[0m in \u001b[0;36mreturn_node_input\u001b[0;34m(input_, Position, Ratio, mode, image_type)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Overlapping Patches Are Not Implemented Yet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mpatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'Patch' referenced before assignment"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "DeSTIN Training/with out Feature extraction\n",
        "Training Iteration Number 0\n",
        "Overlapping Patches Are Not Implemented Yet\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"DesTIN running/Feature Extraction/ over the Training Data\")\n",
      "network_mode = False\n",
      "DESTIN.setmode(network_mode)\n",
      "\n",
      "# Testin it over the training set\n",
      "[data, labels] = loadCifar(10)\n",
      "del labels\n",
      "for I in range(data.shape[0]):  # For Every image in the data set\n",
      "    if I % 1000 == 0:\n",
      "        print(\"Testing Iteration Number %d\" % I)\n",
      "    for L in range(DESTIN.number_of_layers):\n",
      "        if L == 0:\n",
      "            img = data[I][:].reshape(32, 32, 3)\n",
      "            DESTIN.layers[0][L].load_input(img, [4, 4])\n",
      "            DESTIN.layers[0][L].do_layer_learning()\n",
      "        else:\n",
      "            DESTIN.layers[0][L].load_input(\n",
      "                DESTIN.layers[0][L - 1].nodes, [2, 2])\n",
      "            DESTIN.layers[0][L].do_layer_learning()\n",
      "    DESTIN.update_belief_exporter()\n",
      "    if I in range(199, 50999, 200):\n",
      "        Name = 'train/' + str(I + 1) + '.txt'\n",
      "        file_id = open(Name, 'w')\n",
      "        pickle.dump(np.array(DESTIN.network_belief['belief']), file_id)\n",
      "        file_id.close()\n",
      "        # Get rid-off accumulated training beliefs\n",
      "        DESTIN.clean_belief_exporter()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Feature Extraction with the test set\")\n",
      "[data, labels] = loadCifar(6)\n",
      "del labels\n",
      "for I in range(data.shape[0]):  # For Every image in the data set\n",
      "    if I % 1000 == 0:\n",
      "        print(\"Testing Iteration Number %d\" % (I+50000))\n",
      "    for L in range(DESTIN.number_of_layers):\n",
      "        if L == 0:\n",
      "            img = data[I][:].reshape(32, 32, 3)\n",
      "            DESTIN.layers[0][L].load_input(img, [4, 4])\n",
      "            DESTIN.layers[0][L].do_layer_learning()  # Calculates belief for\n",
      "        else:\n",
      "            DESTIN.layers[0][L].load_input(\n",
      "                DESTIN.layers[0][L - 1].nodes, [2, 2])\n",
      "            DESTIN.layers[0][L].do_layer_learning()\n",
      "    DESTIN.update_belief_exporter()\n",
      "    if I in range(199, 10199, 200):\n",
      "        Name = 'test/' + str(I + 1) + '.txt'\n",
      "        file_id = open(Name, 'w')\n",
      "        pickle.dump(np.array(DESTIN.network_belief['belief']), file_id)\n",
      "        file_id.close()\n",
      "        # Get rid-off accumulated training beliefs\n",
      "        DESTIN.clean_belief_exporter()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Training With SVM\"\n",
      "print(\"Loading training and test labels\")\n",
      "[trainData, trainLabel] = loadCifar(10)\n",
      "del trainData\n",
      "[testData, testLabel] = loadCifar(6)\n",
      "del testData"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load Training and Test Data/Extracted from DeSTIN\n",
      "\n",
      "# here we do not use the whole set of feature extracted from DeSTIN\n",
      "# We use the features which are extracted from the top few layers\n",
      "print(\"Loading training and testing features\")\n",
      "trainData = np.array([])\n",
      "for I in range(199, 50000, 200):\n",
      "    Name = 'train/' + str(I + 1) + '.txt'\n",
      "    file_id = open(Name, 'r')\n",
      "    Temp = np.array(pickle.load(file_id))\n",
      "    file_id.close()\n",
      "    trainData = np.hstack((trainData, Temp))\n",
      "del Temp\n",
      "totLen = len(trainData)\n",
      "Width = int(totLen / 50000)\n",
      "trainData = trainData.reshape(50000, Width)\n",
      "trainData = trainData[:, 6400:8500]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Training SVM\n",
      "SVM = svm.LinearSVC(C=10)\n",
      "# C=100, kernel='rbf')\n",
      "print \"Training the SVM\"\n",
      "trainLabel = np.squeeze(np.asarray(trainLabel).reshape(50000, 1))\n",
      "SVM.fit(trainData, trainLabel)\n",
      "print(\"Training Score = %f \" % float(100 * SVM.score(trainData, trainLabel, sample_weight=None)))\n",
      "#print(\"Training Accuracy = %f\" % (SVM.score(trainData, trainLabel) * 100))\n",
      "eff = {}\n",
      "eff['train'] = SVM.score(trainData, trainLabel) * 100\n",
      "del trainData\n",
      "\n",
      "testData = np.array([])\n",
      "for I in range(199, 10000, 200):\n",
      "    Name = 'test/' + str(I + 1) + '.txt'\n",
      "    file_id = open(Name, 'r')\n",
      "    Temp = np.array(pickle.load(file_id))\n",
      "    file_id.close()\n",
      "    testData = np.hstack((testData, Temp))\n",
      "totLen = len(testData)\n",
      "Width = int(totLen / 10000)\n",
      "testData = testData.reshape(10000, Width)\n",
      "testData = testData[:, 6400:8500]\n",
      "del Temp\n",
      "print \"Predicting Test samples\"\n",
      "print(\"Test Score = %f\" % float(100 * SVM.score(testData, testLabel, sample_weight = None)))\n",
      "#print(\"Training Accuracy = %f\" % (SVM.score(testData, testLabel) * 100))\n",
      "eff['test'] = SVM.score(testData, testLabel) * 100\n",
      "io.savemat('accuracy.mat', eff)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}